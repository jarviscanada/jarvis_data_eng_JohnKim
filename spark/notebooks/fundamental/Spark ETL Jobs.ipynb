{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be60341-fe75-47c1-b33c-d7bacb1fe4ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this notebook, you will craft sophisticated ETL jobs that interface with a variety of common data sources, such as \n",
    "- REST APIs (HTTP endpoints)\n",
    "- RDBMS\n",
    "- Hive tables (managed tables)\n",
    "- Various file formats (csv, json, parquet, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9fe8dc-6b2e-4499-8961-7e01309d05f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Interview Questions\n",
    "\n",
    "As you progress through the practice, attempt to answer the following questions:\n",
    "\n",
    "## Columnar File\n",
    "- What is a columnar file format and what advantages does it offer?\n",
    "- Why is Parquet frequently used with Spark and how does it function?\n",
    "- How do you read/write data from/to a Parquet file using a DataFrame?\n",
    "\n",
    "## Partitions\n",
    "- How do you save data to a file system by partitions? (Hint: Provide the code)\n",
    "- How and why can partitions reduce query execution time? (Hint: Give an example)\n",
    "\n",
    "## JDBC and RDBMS\n",
    "- How do you load data from an RDBMS into Spark? (Hint: Discuss the steps and JDBC)\n",
    "\n",
    "## REST API and HTTP Requests\n",
    "- How can Spark be used to fetch data from a REST API? (Hint: Discuss making API requests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c7f0dcb-2214-41ae-a6f4-12d5a34506ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job One: Parquet file\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Data transformation requirements https://pgexercises.com/questions/aggregates/fachoursbymonth.html\n",
    "\n",
    "### Load\n",
    "Load data into a parquet file\n",
    "\n",
    "### What is Parquet? \n",
    "\n",
    "Columnar files are an important technique for optimizing Spark queries. Additionally, they are often tested in interviews.\n",
    "- https://www.youtube.com/watch?v=KLFadWdomyI\n",
    "- https://www.databricks.com/glossary/what-is-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33324d02-bc67-4c31-b822-8fe8c69fead5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n|facid|Total Slots|\n+-----+-----------+\n|    0|       1320|\n|    1|       1278|\n|    2|       1209|\n|    3|        830|\n|    4|       1404|\n|    5|        228|\n|    6|       1104|\n|    7|        908|\n|    8|        911|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col\n",
    "\n",
    "bookings_df = spark.table(\"bookings\")\n",
    "members_df = spark.table(\"members\")\n",
    "facilities_df = spark.table(\"facilities\")\n",
    "\n",
    "result_df = (\n",
    "    bookings_df.groupBy(\"facid\")\n",
    "    .agg(sum(col(\"slots\")).alias(\"Total Slots\"))\n",
    "    .orderBy(\"facid\") \n",
    ")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "output_path = \"/data/slots\"\n",
    "\n",
    "result_df.write.parquet(output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b51d425e-d532-47e5-8cbf-a91ca78246b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Two: Partitions\n",
    "\n",
    "### Extract\n",
    "Extract data from the managed tables (e.g. `bookings_csv`, `members_csv`, and `facilities_csv`)\n",
    "\n",
    "### Transform\n",
    "Transform the data https://pgexercises.com/questions/joins/threejoin.html\n",
    "\n",
    "### Load\n",
    "Partition the result data by facility column and then save to `threejoin_delta` managed table. Additionally, they are often tested in interviews.\n",
    "\n",
    "hint: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html\n",
    "\n",
    "What are paritions? \n",
    "\n",
    "Partitions are an important technique to optimize Spark queries\n",
    "- https://www.youtube.com/watch?v=hvF7tY2-L3U&t=268s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32aea2ca-5178-4034-91ee-c09942c5f518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n| facility_name|   member_name|\n+--------------+--------------+\n|Tennis Court 1|    Anne Baker|\n|Tennis Court 2|    Anne Baker|\n|Tennis Court 1|  Burton Tracy|\n|Tennis Court 2|  Burton Tracy|\n|Tennis Court 1|  Charles Owen|\n|Tennis Court 2|  Charles Owen|\n|Tennis Court 2|  Darren Smith|\n|Tennis Court 1| David Farrell|\n|Tennis Court 2| David Farrell|\n|Tennis Court 1|   David Jones|\n|Tennis Court 2|   David Jones|\n|Tennis Court 1|  David Pinker|\n|Tennis Court 1| Douglas Jones|\n|Tennis Court 1| Erica Crumpet|\n|Tennis Court 1|Florence Bader|\n|Tennis Court 2|Florence Bader|\n|Tennis Court 1|   GUEST GUEST|\n|Tennis Court 2|   GUEST GUEST|\n|Tennis Court 1|Gerald Butters|\n|Tennis Court 2|Gerald Butters|\n+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "\n",
    "result_df = (bookings_df.join(facilities_df, col(\"bookings.facid\") == col(\"facilities.facid\")).join(members_df, col(\"bookings.memid\") == col(\"members.memid\")))\n",
    "result_df = result_df.filter(col(\"name\").isin(\"Tennis Court 1\", \"Tennis Court 2\")).select(\n",
    "        col(\"name\").alias(\"facility_name\"), \n",
    "        concat_ws(\" \", col(\"firstname\"), col(\"surname\")).alias(\"member_name\")\n",
    "    ).distinct().orderBy(\"member_name\", \"facility_name\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "output_path = \"/data/facility\"\n",
    "\n",
    "result_df.write.partitionBy(\"facility_name\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7610de14-acd6-4374-945d-661dbc08a08e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Three: HTTP Requests\n",
    "\n",
    "### Extract\n",
    "Extract daily stock price data price from the following companies, Google, Apple, Microsoft, and Tesla. \n",
    "\n",
    "Data Source\n",
    "- API: https://rapidapi.com/alphavantage/api/alpha-vantage\n",
    "- Endpoint: GET `TIME_SERIES_DAILY`\n",
    "\n",
    "Sample HTTP request\n",
    "\n",
    "```\n",
    "curl --request GET \\\n",
    "\t--url 'https://alpha-vantage.p.rapidapi.com/query?function=TIME_SERIES_DAILY&symbol=TSLA&outputsize=compact&datatype=json' \\\n",
    "\t--header 'X-RapidAPI-Host: alpha-vantage.p.rapidapi.com' \\\n",
    "\t--header 'X-RapidAPI-Key: [YOUR_KEY]'\n",
    "\n",
    "```\n",
    "\n",
    "Sample Python HTTP request\n",
    "\n",
    "```\n",
    "import requests\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "\n",
    "querystring = {\n",
    "    \"function\":\"TIME_SERIES_DAILY\",\n",
    "    \"symbol\":\"IBM\",\n",
    "    \"datatype\":\"json\",\n",
    "    \"outputsize\":\"compact\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"[YOUR_KEY]\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=querystring)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Now 'data' contains the daily time series data for \"IBM\"\n",
    "```\n",
    "\n",
    "### Transform\n",
    "Find **weekly** max closing price for each company.\n",
    "\n",
    "hints: \n",
    "  - Use a `for-loop` to get stock data for each company\n",
    "  - Use the spark `union` operation to concat all data into one DF\n",
    "  - create a new `week` column from the data column\n",
    "  - use `group by` to calcualte max closing price\n",
    "\n",
    "### Load\n",
    "- Partition `DF` by company\n",
    "- Load the DF in to a managed table called, `max_closing_price_weekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b76fcc5-fc12-4401-a16c-e24c4c890dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n|      date|company|    high|\n+----------+-------+--------+\n|2025-03-18|   GOOG|168.4600|\n|2025-03-18|   TSLA|245.4000|\n|2025-03-18|   MSFT|392.7050|\n|2025-03-18|   AAPL|215.2200|\n|2025-03-14|   TSLA|253.3700|\n|2025-03-14|   MSFT|390.2300|\n|2025-03-14|   GOOG|170.4500|\n|2025-03-14|   AAPL|236.1600|\n|2025-03-07|   TSLA|303.9400|\n|2025-03-07|   MSFT|402.1500|\n|2025-03-07|   GOOG|176.9000|\n|2025-03-07|   AAPL|244.0272|\n|2025-02-28|   TSLA|342.3973|\n|2025-02-28|   MSFT|409.3700|\n|2025-02-28|   GOOG|185.0900|\n|2025-02-28|   AAPL|250.0000|\n|2025-02-21|   TSLA|367.3400|\n|2025-02-21|   MSFT|419.3100|\n|2025-02-21|   GOOG|187.7800|\n|2025-02-21|   AAPL|248.6900|\n+----------+-------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "url = \"https://alpha-vantage.p.rapidapi.com/query\"\n",
    "headers = {\n",
    "    \"X-RapidAPI-Host\": \"alpha-vantage.p.rapidapi.com\",\n",
    "    \"X-RapidAPI-Key\": \"9f33b7490bmsh607e6ec0a839621p1c6ab9jsn91d5f9c379ad\"\n",
    "}\n",
    "\n",
    "tickers = [\"GOOG\", \"AAPL\", \"MSFT\", \"TSLA\"]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"company\", StringType(), True),\n",
    "    StructField(\"high\", StringType(), True)\n",
    "])\n",
    "\n",
    "result_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "\n",
    "for element in tickers:\n",
    "    querystring = {\n",
    "        \"function\": \"TIME_SERIES_WEEKLY\",\n",
    "        \"symbol\": element,\n",
    "        \"datatype\": \"json\",\n",
    "        \"outputsize\": \"compact\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=querystring)\n",
    "    data = response.json()\n",
    "    weekly_data = data['Weekly Time Series']\n",
    "\n",
    "    data_list = []\n",
    "    for date, values in weekly_data.items():\n",
    "        data_list.append((date, element, values[\"2. high\"])) \n",
    "\n",
    "    ticker_df = spark.createDataFrame(data_list, schema=schema) \n",
    "    result_df = result_df.union(ticker_df)\n",
    "\n",
    "result_df = result_df.orderBy(col(\"date\").desc())\n",
    "\n",
    "# Save as a managed table with partitioning by company\n",
    "result_df.write.mode(\"overwrite\").partitionBy(\"company\").saveAsTable(\"max_closing_price_weekly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f078f25-be18-4d6a-bd4a-930b496f8c90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n|      date|company|    high|\n+----------+-------+--------+\n|2025-03-18|   AAPL|215.2200|\n|2025-03-14|   AAPL|236.1600|\n|2025-03-07|   AAPL|244.0272|\n|2025-02-28|   AAPL|250.0000|\n|2025-02-21|   AAPL|248.6900|\n|2025-02-14|   AAPL|245.5500|\n|2025-02-07|   AAPL|234.0000|\n|2025-01-31|   AAPL|247.1900|\n|2025-01-24|   AAPL|227.0300|\n|2025-01-17|   AAPL|238.9600|\n|2025-01-10|   AAPL|247.3300|\n|2025-01-03|   AAPL|253.5000|\n|2024-12-27|   AAPL|260.1000|\n|2024-12-20|   AAPL|255.0000|\n|2024-12-13|   AAPL|250.8000|\n|2024-12-06|   AAPL|244.6300|\n|2024-11-29|   AAPL|237.8100|\n|2024-11-22|   AAPL|230.7199|\n|2024-11-15|   AAPL|228.8700|\n|2024-11-08|   AAPL|228.6600|\n+----------+-------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM max_closing_price_weekly\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f98592-1f5f-4b42-9350-6720e69a7c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ETL Job Four: RDBMS\n",
    "\n",
    "\n",
    "### Extract\n",
    "Extract RNA data from a public PostgreSQL database.\n",
    "\n",
    "- https://rnacentral.org/help/public-database\n",
    "- Extract 100 RNA records from the `rna` table (hint: use `limit` in your sql)\n",
    "- hint: use `spark.read.jdbc` https://docs.databricks.com/external-data/jdbc.html\n",
    "\n",
    "### Transform\n",
    "We want to load the data as it so there is no transformation required.\n",
    "\n",
    "\n",
    "### Load\n",
    "Load the DF in to a managed table called, `rna_100_records`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3011d775-d108-4cb0-85d1-bf21ae1c23d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Hostname =  \"hh-pgsql-public.ebi.ac.uk\"\n",
    "Port = \"5432\"\n",
    "Database = \"pfmegrnargs\"\n",
    "User = \"reader\"\n",
    "Password = \"NWDMCE5xdipIjRrp\"\n",
    "\n",
    "employees_table = (spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\n",
    "  .option(\"url\", \"jdbc:postgresql://\" + Hostname + \":\" + Port + \"/\" + Database)\n",
    "  .option(\"dbtable\", \"rna\")\n",
    "  .option(\"user\", User)\n",
    "  .option(\"password\", Password)\n",
    "  .load()\n",
    ")\n",
    "\n",
    "employees_table = employees_table.limit(100)\n",
    "\n",
    "employees_table.write.mode(\"overwrite\").saveAsTable(\"rna_100_records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a271aeb5-114e-4f4d-8102-85192b29254c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n|      id|          upi|           timestamp|userstamp|           crc64| len|           seq_short|seq_long|                 md5|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\n| 8988357|URS00008926C5| 2015-10-20 18:04:07|   RNACEN|F9626977AB4E17FB|1336|TCAGCGGCGAACGGGTG...|    null|fe4792a9218a34fde...|\n| 8988360|URS00008926C8| 2015-10-20 18:04:07|   RNACEN|DEA611A8ABDE9078|1307|ACTGCTATCGGATTGAT...|    null|5eb946fc85a2e16f4...|\n| 8988361|URS00008926C9| 2015-10-20 18:04:07|   RNACEN|AE161A21AF6713C0|1367|AGCCCAGCTTGCTGGGT...|    null|fe4849b1977b5be3c...|\n| 8988362|URS00008926CA| 2015-10-20 18:04:07|   RNACEN|03DF15DE82E78D7F|1398|GAGTTTGATCATGGCTC...|    null|c4bb7b410de36a58c...|\n| 8988364|URS00008926CC| 2015-10-20 18:04:07|   RNACEN|AE0439B061E1640E|1409|GTCGAACGGTAACAGGA...|    null|5ebae3845f9c331ae...|\n|11797345|URS0000B40361|2017-10-13 16:48:...|   rnacen|7DEEC8E7492E0C07| 426|GTGAGGAATATTGGTCA...|    null|1a56bf4ac54397a3f...|\n| 8987869|URS00008924DD| 2015-10-20 18:04:07|   RNACEN|501C63AF85F4994B|1378|TGATCCTGGCGCAGGAT...|    null|c449606151afed796...|\n| 8987870|URS00008924DE| 2015-10-20 18:04:07|   RNACEN|E3CAEB752E19EF2A|1370|TTAGAGTTTGATCCTGG...|    null|5e6050447aa007745...|\n| 8988018|URS0000892572| 2015-10-20 18:04:07|   RNACEN|9FEF66572DBF356C|1400|CAGGATGAACGCTAGCG...|    null|5e7ee4bb62cfcdd77...|\n| 8988488|URS0000892748| 2015-10-20 18:04:07|   RNACEN|FA606F4B0ABA4DC4|1478|GAGTTTGATCCTGGCTC...|    null|b7bcd6b9fbd22a1e8...|\n| 8988492|URS000089274C| 2015-10-20 18:04:07|   RNACEN|83F1CFEB52A7A8AB|1382|TCATGGCTCAGGACGAA...|    null|b7be584d9de7bfead...|\n|11797346|URS0000B40362|2017-10-13 16:48:...|   rnacen|69A87BE52361CDE2| 253|TACGAAGGATGCAAGCG...|    null|1a56c35f320afd3e1...|\n| 8988495|URS000089274F| 2015-10-20 18:04:07|   RNACEN|51D37F4799755FEE|1343|GACGAACGCTGGCGGCG...|    null|5ece2db19192edcfa...|\n| 8988497|URS0000892751| 2015-10-20 18:04:07|   RNACEN|67449140B80EE869|1352|CTGGCGGCGTGCTTAAC...|    null|b7bfcc1f663fcdf92...|\n| 8988498|URS0000892752| 2015-10-20 18:04:07|   RNACEN|F8D635CB8DD7CAE9|1343|GACGAACGCTTGCGGCG...|    null|c4d9ab82753b884ef...|\n| 8988499|URS0000892753| 2015-10-20 18:04:07|   RNACEN|802C8DE37D44AE17|1421|AGAGTTTGATCCTGGCT...|    null|5ece8ffab9c6b6049...|\n| 8988502|URS0000892756| 2015-10-20 18:04:07|   RNACEN|14D18A222F44A6BE|1462|ACGAACGCTGGCGGCGT...|    null|c4d9e533ef6cb445d...|\n|11884212|URS0000B556B4|2017-10-13 16:48:...|   rnacen|B6102A48637B6218|  93|GGAGAGATGGCCGAGTG...|    null|3f3b312f114ac937f...|\n|12686420|URS0000C19454|2017-10-19 09:46:...|   rnacen|0857A1E718F4399A| 124|CTAGTACTTCCTCCGTC...|    null|28e0ec929dbfd71f4...|\n| 8988503|URS0000892757| 2015-10-20 18:04:07|   RNACEN|B96FDAB291A46D5E|1431|CAGGACGAACGCTGGCG...|    null|5ed407618693a95ab...|\n+--------+-------------+--------------------+---------+----------------+----+--------------------+--------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM rna_100_records\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2 - Spark ETL Jobs Exercieses",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}